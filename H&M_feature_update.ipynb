{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 52nd Place Solution Notebook\n",
    "\n",
    "This notebook is a cleaned version of my final submission.  \n",
    "See [this post](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/324076/) for some details about my solution.\n",
    "\n",
    "The notebook has minimal code in it - most of the code is imported from my [handmhelpers dataset](https://www.kaggle.com/datasets/jacob34/handmhelpers), which is synced to [this github repo](https://github.com/JacobCP/kaggle-handm-helpers) .  \n",
    "See [this post](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/324078) for some details about my code development.  \n",
    "\n",
    "**Please note:**  \n",
    "I plan on continuing to update the github repo, as I try to recreate some of the strategies shared by winning teams.  \n",
    "Some of those changes may break the code usage for this notebook.  \n",
    "In order to keep this notebook functional, I will no longer be updating the dataset to reflect the changes made to the repo - it will remain at commit 86c412e902a7692b24e15791322a8dfeb5a761eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:40:13.334583Z",
     "iopub.status.busy": "2025-12-18T06:40:13.334366Z",
     "iopub.status.idle": "2025-12-18T06:40:17.788268Z",
     "shell.execute_reply": "2025-12-18T06:40:17.787522Z",
     "shell.execute_reply.started": "2025-12-18T06:40:13.334514Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import pickle as pkl\n",
    "import shelve\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "    \n",
    "sys.path.append(\"../input/\")\n",
    "from handmhelpers import io as h_io, sub as h_sub, cv as h_cv, fe as h_fe\n",
    "from handmhelpers import modeling as h_modeling, candidates as h_can, pairs as h_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "def patched_day_week_numbers(dates: cudf.Series):\n",
    "    pd_dates = cudf.to_datetime(dates)\n",
    "    unique_dates = cudf.Series(pd_dates.unique())\n",
    "    numbered_days = unique_dates - unique_dates.min() + timedelta(1)\n",
    "    numbered_days = numbered_days.dt.days\n",
    "    extra_days = numbered_days.max() % 7\n",
    "    numbered_days -= extra_days\n",
    "    day_weeks = (numbered_days + 6) // 7  # không dùng applymap\n",
    "    day_weeks_map = cudf.DataFrame({\"day_weeks\": day_weeks, \"unique_dates\": unique_dates}).set_index(\"unique_dates\")[\"day_weeks\"]\n",
    "    all_day_weeks = pd_dates.map(day_weeks_map).astype(\"int8\")\n",
    "    return all_day_weeks\n",
    "\n",
    "import handmhelpers.fe as h_fe\n",
    "h_fe.day_week_numbers = patched_day_week_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:40:24.685261Z",
     "iopub.status.busy": "2025-12-18T06:40:24.684987Z",
     "iopub.status.idle": "2025-12-18T06:41:23.958561Z",
     "shell.execute_reply": "2025-12-18T06:41:23.957731Z",
     "shell.execute_reply.started": "2025-12-18T06:40:24.685228Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c, t, a = h_io.load_data(files=['customers.csv', 'transactions_train.csv', 'articles.csv'])        \n",
    "\n",
    "index_to_id_dict_path = h_fe.reduce_customer_id_memory(c, [t])\n",
    "t[\"week_number\"] = h_fe.day_week_numbers(t[\"t_dat\"])\n",
    "t[\"t_dat\"] = h_fe.day_numbers(t[\"t_dat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get item pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:42:13.722199Z",
     "iopub.status.busy": "2025-12-18T06:42:13.721943Z",
     "iopub.status.idle": "2025-12-18T06:43:18.098542Z",
     "shell.execute_reply": "2025-12-18T06:43:18.097767Z",
     "shell.execute_reply.started": "2025-12-18T06:42:13.722172Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pairs_per_item = 5\n",
    "\n",
    "week_number_pairs = {}\n",
    "for week_number in [96, 97, 98, 99, 100, 101, 102, 103, 104]:\n",
    "    print(f\"Creating pairs for week number {week_number}\")\n",
    "    week_number_pairs[week_number] = h_pairs.create_pairs(\n",
    "        t, week_number, pairs_per_item, verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main retrieval/features function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:43:18.152479Z",
     "iopub.status.busy": "2025-12-18T06:43:18.151985Z",
     "iopub.status.idle": "2025-12-18T06:43:18.167975Z",
     "shell.execute_reply": "2025-12-18T06:43:18.167334Z",
     "shell.execute_reply.started": "2025-12-18T06:43:18.152443Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_candidates_with_features_df(t, c, a, customer_batch=None, **kwargs):\n",
    "    # splitting cv\n",
    "    features_df, label_df = h_cv.feature_label_split(\n",
    "        t, kwargs[\"label_week\"], kwargs[\"feature_periods\"]\n",
    "    )\n",
    "    \n",
    "    # converting relative day_number\n",
    "    features_df[\"t_dat\"] = h_fe.how_many_ago(features_df[\"t_dat\"])\n",
    "    features_df[\"week_number\"] = h_fe.how_many_ago(features_df[\"week_number\"])\n",
    "    \n",
    "    # pull out the cv week\n",
    "    article_pairs_df = week_number_pairs[kwargs[\"label_week\"] - 1]\n",
    "    \n",
    "    # check if we can limit customers\n",
    "    if len(label_df) > 0:\n",
    "        customers = label_df[\"customer_id\"].unique()\n",
    "    elif customer_batch is not None:\n",
    "        customers = customer_batch\n",
    "    else:\n",
    "        customers = None\n",
    "    \n",
    "    ############################################\n",
    "    # creating candidates (and adding features)\n",
    "    ###########################################\n",
    "    \n",
    "    features_db = {}\n",
    "    \n",
    "    # creating candidate (and saving features created)\n",
    "    recent_customer_cand, features_db[\"customer_article\"] = (\n",
    "        h_can.create_recent_customer_candidates(\n",
    "            features_df,\n",
    "            kwargs[\"ca_num_weeks\"],\n",
    "            customers=customers,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        cust_last_week_cand,\n",
    "        cust_last_week_pair_cand,\n",
    "        features_db[\"clw\"],\n",
    "        features_db[\"clw_pairs\"],\n",
    "    ) = h_can.create_last_customer_weeks_and_pairs(\n",
    "        features_df,\n",
    "        article_pairs_df,\n",
    "        kwargs[\"clw_num_weeks\"],\n",
    "        kwargs[\"clw_num_pair_weeks\"],\n",
    "        customers=customers,\n",
    "    )\n",
    "    \n",
    "    _, features_db[\"popular_articles\"] = h_can.create_popular_article_cand(\n",
    "        features_df,\n",
    "        c,\n",
    "        a,\n",
    "        kwargs[\"pa_num_weeks\"],\n",
    "        kwargs[\"hier_col\"],\n",
    "        num_candidates=kwargs[\"num_recent_candidates\"],\n",
    "        num_articles=kwargs[\"num_recent_articles\"],\n",
    "        customers=customers,\n",
    "    )\n",
    "    (\n",
    "        age_bucket_can,\n",
    "        age_bucket_cust_features,\n",
    "        age_bucket_pair_features,\n",
    "    ) = h_can.create_age_bucket_candidates(\n",
    "        features_df,\n",
    "        c,\n",
    "        kwargs[\"num_age_buckets\"],\n",
    "        articles=kwargs[\"num_recent_articles\"],\n",
    "        customers=customers,\n",
    "    )\n",
    "    features_db[\"age_bucket\"] = age_bucket_pair_features\n",
    "\n",
    "    # build source/rule features\n",
    "    def build_rule_part(cand_df, feature_tuple, score_col, rule_name):\n",
    "        feature_df = feature_tuple[1].reset_index()[\n",
    "            [\"customer_id\", \"article_id\", score_col]\n",
    "        ]\n",
    "        tmp = cand_df.merge(feature_df, on=[\"customer_id\", \"article_id\"], how=\"left\")\n",
    "        tmp = tmp.rename(columns={score_col: \"rule_score\"})\n",
    "        tmp[\"rule_score\"] = tmp[\"rule_score\"].fillna(-1)\n",
    "        tmp[\"rule\"] = rule_name\n",
    "        return tmp[[\"customer_id\", \"article_id\", \"rule\", \"rule_score\"]]\n",
    "\n",
    "    rule_parts = [\n",
    "        build_rule_part(\n",
    "            recent_customer_cand,\n",
    "            features_db[\"customer_article\"],\n",
    "            \"ca_purchase_count\",\n",
    "            \"recent\",\n",
    "        ),\n",
    "        build_rule_part(\n",
    "            cust_last_week_cand,\n",
    "            features_db[\"clw\"],\n",
    "            \"ca_count\",\n",
    "            \"last_weeks\",\n",
    "        ),\n",
    "        build_rule_part(\n",
    "            cust_last_week_pair_cand,\n",
    "            features_db[\"clw_pairs\"],\n",
    "            \"pair_lift\",\n",
    "            \"pairs\",\n",
    "        ),\n",
    "        build_rule_part(\n",
    "            age_bucket_can,\n",
    "            features_db[\"age_bucket\"],\n",
    "            \"article_bucket_count\",\n",
    "            \"age_bucket\",\n",
    "        ),\n",
    "    ]\n",
    "    rule_df = cudf.concat(rule_parts)\n",
    "    rule_df = rule_df.sort_values(\n",
    "        [\"rule\", \"customer_id\", \"rule_score\"], ascending=[True, True, False]\n",
    "    )\n",
    "    rule_df[\"rank_within_rule\"] = rule_df.groupby([\"rule\", \"customer_id\"]).cumcount()\n",
    "\n",
    "    rule_features_df = (\n",
    "        rule_df.groupby([\"customer_id\", \"article_id\"])\n",
    "        .agg({\"rule\": \"nunique\", \"rule_score\": \"max\", \"rank_within_rule\": \"min\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    rule_features_df.columns = [\n",
    "        \"customer_id\",\n",
    "        \"article_id\",\n",
    "        \"n_sources\",\n",
    "        \"best_rule_score\",\n",
    "        \"best_rank_within_rule\",\n",
    "    ]\n",
    "\n",
    "    for rule_name in rule_df[\"rule\"].unique().to_pandas():\n",
    "        flag_df = rule_df[rule_df[\"rule\"] == rule_name][\n",
    "            [\"customer_id\", \"article_id\"]\n",
    "        ].drop_duplicates()\n",
    "        flag_df[f\"{rule_name}_flag\"] = 1\n",
    "        rule_features_df = rule_features_df.merge(\n",
    "            flag_df, how=\"left\", on=[\"customer_id\", \"article_id\"]\n",
    "        )\n",
    "        rule_features_df[f\"{rule_name}_flag\"] = (\n",
    "            rule_features_df[f\"{rule_name}_flag\"].fillna(0).astype(\"int8\")\n",
    "        )\n",
    "\n",
    "    # features_db[\"rule_features\"] = (\n",
    "    #     [\"customer_id\", \"article_id\"],\n",
    "    #     rule_features_df.set_index([\"customer_id\", \"article_id\"]),\n",
    "    # )\n",
    "\n",
    "    cand = [\n",
    "        recent_customer_cand,\n",
    "        cust_last_week_cand,\n",
    "        cust_last_week_pair_cand,\n",
    "        age_bucket_can,\n",
    "    ]\n",
    "    cand = cudf.concat(cand).drop_duplicates()\n",
    "    cand = cand.sort_values([\"customer_id\", \"article_id\"]).reset_index(drop=True)\n",
    "    \n",
    "    del recent_customer_cand, cust_last_week_cand, cust_last_week_pair_cand, age_bucket_can\n",
    "    \n",
    "    cand = h_can.filter_candidates(cand, t, **kwargs)\n",
    "    \n",
    "    # creating other features\n",
    "    h_fe.create_cust_hier_features(features_df, a, kwargs[\"hier_cols\"], features_db)\n",
    "    h_fe.create_cust_hier_decay_features(\n",
    "        features_df,\n",
    "        a,\n",
    "        kwargs[\"hier_cols\"],\n",
    "        features_db,\n",
    "        decay_gamma=kwargs.get(\"hier_decay_gamma\", 0.3),\n",
    "    )\n",
    "        # sau khi đã gọi create_cust_hier_decay_features\n",
    "    for k, v in list(features_db.items()):\n",
    "        if k.endswith(\"_decay_features\"):\n",
    "            hier = k[len(\"cust_\"):-len(\"_decay_features\")]\n",
    "            cols, df = v\n",
    "            df = df.rename(\n",
    "                columns={\n",
    "                    \"last_seen_category_weeks_ago\": f\"last_seen_{hier}_weeks_ago\"\n",
    "                }\n",
    "            )\n",
    "            features_db[k] = (cols, df)\n",
    "\n",
    "    h_fe.create_price_features(features_df, features_db)\n",
    "    h_fe.create_cust_features(c, features_db)\n",
    "    h_fe.create_article_cust_features(features_df, c, features_db)\n",
    "    h_fe.create_lag_features(features_df, a, kwargs[\"lag_days\"], features_db)\n",
    "    h_fe.create_rebuy_features(features_df, features_db)\n",
    "    h_fe.create_cust_t_features(features_df, a, features_db)\n",
    "    h_fe.create_art_t_features(features_df, features_db)\n",
    "    \n",
    "    del features_df\n",
    "\n",
    "    # another filter at the end, for the ones that didn't get filtered earlier\n",
    "    if customers is not None:\n",
    "        cand = cand[cand[\"customer_id\"].isin(customers)]\n",
    "    \n",
    "    # report on recall/precision of candidates\n",
    "    if kwargs[\"cv\"]:\n",
    "        ground_truth_candidates = label_df[[\"customer_id\", \"article_id\"]].drop_duplicates()\n",
    "        h_cv.report_candidates(cand, ground_truth_candidates)\n",
    "        del ground_truth_candidates        \n",
    "    \n",
    "    # adding features to candidates\n",
    "    cand_with_f_df = h_can.add_features_to_candidates(\n",
    "        cand, features_db, c, a\n",
    "    )\n",
    "\n",
    "    cand_with_f_df = cand_with_f_df.merge(\n",
    "        rule_features_df, how=\"left\", on=[\"customer_id\", \"article_id\"]\n",
    "    )\n",
    "    \n",
    "    # manually adding article features (couldn't use shelve for some reason)\n",
    "    for article_col in kwargs[\"article_columns\"]:\n",
    "        art_col_map = a.set_index(\"article_id\")[article_col]\n",
    "        cand_with_f_df[article_col] = cand_with_f_df[\"article_id\"].map(art_col_map)\n",
    "        \n",
    "    # ép mọi cột không phải số sang mã category để model nhận numeric\n",
    "    for col in cand_with_f_df.columns:\n",
    "        if col in [\"customer_id\", \"article_id\"]:\n",
    "            continue\n",
    "        if str(cand_with_f_df[col].dtype) not in [\n",
    "            \"int8\",\"int16\",\"int32\",\"int64\",\n",
    "            \"float16\",\"float32\",\"float64\",\n",
    "            \"bool\"\n",
    "        ]:\n",
    "            cand_with_f_df[col] = (\n",
    "                cand_with_f_df[col].astype(\"category\").cat.codes.astype(\"float32\")\n",
    "            )\n",
    "\n",
    "    \n",
    "    # limiting features\n",
    "    if kwargs[\"selected_features\"] is not None:\n",
    "        cand_with_f_df = cand_with_f_df[\n",
    "            [\"customer_id\", \"article_id\"] + kwargs[\"selected_features\"]\n",
    "        ]\n",
    "        \n",
    "    \n",
    "    \n",
    "    assert len(cand) == len(cand_with_f_df), \"seem to have duplicates in the feature dfs\"\n",
    "    del cand\n",
    "    \n",
    "    return cand_with_f_df, label_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:43:18.169006Z",
     "iopub.status.busy": "2025-12-18T06:43:18.168826Z",
     "iopub.status.idle": "2025-12-18T06:43:18.180162Z",
     "shell.execute_reply": "2025-12-18T06:43:18.17956Z",
     "shell.execute_reply.started": "2025-12-18T06:43:18.168981Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_model_score(ids_df, preds, truth_df):\n",
    "    predictions = h_modeling.create_predictions(ids_df, preds)\n",
    "    true_labels = h_cv.ground_truth(truth_df).set_index(\"customer_id\")[\"prediction\"]\n",
    "    score = round(h_cv.comp_average_precision(true_labels, predictions),5)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters - one place for all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:43:18.182084Z",
     "iopub.status.busy": "2025-12-18T06:43:18.181835Z",
     "iopub.status.idle": "2025-12-18T06:43:18.190818Z",
     "shell.execute_reply": "2025-12-18T06:43:18.190209Z",
     "shell.execute_reply.started": "2025-12-18T06:43:18.182045Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    \"cv\": True,\n",
    "    \"feature_periods\": 105,\n",
    "    \"label_week\": 104,\n",
    "    \"index_to_id_dict_path\": index_to_id_dict_path,\n",
    "    \"pairs_file_version\": \"_v3_5_ex\",\n",
    "    \"num_recent_candidates\": 36,\n",
    "    \"num_recent_articles\": 12,\n",
    "    \"hier_col\": \"department_no\",\n",
    "    \"ca_num_weeks\": 3,\n",
    "    \"clw_num_weeks\": 12,\n",
    "    \"clw_num_pair_weeks\": 2,\n",
    "    \"pa_num_weeks\": 1,\n",
    "    \"num_age_buckets\": 4,\n",
    "    \"filter_recent_art_weeks\": 1,\n",
    "    \"filter_num_articles\": None,\n",
    "    \"lag_days\": [1, 3, 7, 14, 28],\n",
    "    \"article_columns\": [\"index_code\"],\n",
    "    \"hier_cols\": [\n",
    "        \"department_no\", \"section_no\", \"index_group_no\", \"index_code\",\n",
    "        \"product_type_no\", \"product_group_name\"\n",
    "    ],\n",
    "    \"hier_decay_gamma\": 0.3,\n",
    "    \"selected_features\": None,\n",
    "    \"lgbm_params\": {\"n_estimators\": 200, \"num_leaves\": 20},\n",
    "    \"log_evaluation\": 10,\n",
    "    \"early_stopping\": 20,\n",
    "    \"eval_at\": 12,\n",
    "    \"save_model\": True,\n",
    "    \"num_concats\": 5,\n",
    "}\n",
    "sub_params = {\n",
    "    \"cv\": False,\n",
    "    \"feature_periods\": 105,\n",
    "    \"label_week\": 105,\n",
    "    \"index_to_id_dict_path\": index_to_id_dict_path,\n",
    "    \"pairs_file_version\": \"_v3_5_ex\",\n",
    "    \"num_recent_candidates\": 60,\n",
    "    \"num_recent_articles\": 12,\n",
    "    \"hier_col\": \"department_no\",\n",
    "    \"ca_num_weeks\": 3,\n",
    "    \"clw_num_weeks\": 12,\n",
    "    \"clw_num_pair_weeks\": 2,\n",
    "    \"pa_num_weeks\": 1,\n",
    "    \"num_age_buckets\": 4,\n",
    "    \"filter_recent_art_weeks\": 1,\n",
    "    \"filter_num_articles\": None,\n",
    "    \"lag_days\": [1, 3, 7, 14, 28],\n",
    "    \"article_columns\": [\"index_code\"],\n",
    "    \"hier_cols\": [\n",
    "        \"department_no\", \"section_no\", \"index_group_no\", \"index_code\",\n",
    "        \"product_type_no\", \"product_group_name\"\n",
    "    ],\n",
    "    \"hier_decay_gamma\": 0.3,\n",
    "    \"selected_features\": None,\n",
    "    \"lgbm_params\": {\n",
    "        \"n_estimators\": 150,\n",
    "        \"num_leaves\": 20,    \n",
    "    },\n",
    "    \"log_evaluation\": 10,\n",
    "    \"eval_at\": 12,\n",
    "    \"prediction_models\": [\"model_104\", \"model_105\"],\n",
    "    \"save_model\": True,\n",
    "    \"num_concats\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:43:18.191917Z",
     "iopub.status.busy": "2025-12-18T06:43:18.191686Z",
     "iopub.status.idle": "2025-12-18T06:43:18.201831Z",
     "shell.execute_reply": "2025-12-18T06:43:18.201065Z",
     "shell.execute_reply.started": "2025-12-18T06:43:18.191883Z"
    }
   },
   "outputs": [],
   "source": [
    "cand_features_func = create_candidates_with_features_df\n",
    "scoring_func = calculate_model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:43:18.202862Z",
     "iopub.status.busy": "2025-12-18T06:43:18.20268Z",
     "iopub.status.idle": "2025-12-18T06:45:39.78658Z",
     "shell.execute_reply": "2025-12-18T06:45:39.785866Z",
     "shell.execute_reply.started": "2025-12-18T06:43:18.20284Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_weeks = [104]\n",
    "results = h_modeling.run_all_cvs(\n",
    "    t, c, a, cand_features_func, scoring_func, \n",
    "    cv_weeks=cv_weeks, **cv_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:45:39.789741Z",
     "iopub.status.busy": "2025-12-18T06:45:39.789122Z",
     "iopub.status.idle": "2025-12-18T06:52:37.208384Z",
     "shell.execute_reply": "2025-12-18T06:52:37.207729Z",
     "shell.execute_reply.started": "2025-12-18T06:45:39.789706Z"
    }
   },
   "outputs": [],
   "source": [
    "from cuml.fil import ForestInference as _FI\n",
    "\n",
    "_real_load = _FI.load\n",
    "\n",
    "def _load_compat(*args, output_class=None, is_classifier=None, **kwargs):\n",
    "    # helper cũ truyền output_class -> map sang is_classifier\n",
    "    if is_classifier is None and output_class is not None:\n",
    "        is_classifier = output_class\n",
    "    return _real_load(*args, is_classifier=is_classifier, **kwargs)\n",
    "\n",
    "_FI.load = staticmethod(_load_compat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*Parameter `output_class` was deprecated.*\",\n",
    "    category=FutureWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "h_modeling.full_sub_train_run(t, c, a, cand_features_func, scoring_func, **sub_params)\n",
    "predictions = h_modeling.full_sub_predict_run(\n",
    "    t, c, a, cand_features_func, **sub_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:52:37.210085Z",
     "iopub.status.busy": "2025-12-18T06:52:37.209814Z",
     "iopub.status.idle": "2025-12-18T06:53:40.046536Z",
     "shell.execute_reply": "2025-12-18T06:53:40.045775Z",
     "shell.execute_reply.started": "2025-12-18T06:52:37.210049Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = h_sub.create_sub(c[\"customer_id\"], predictions, index_to_id_dict_path)\n",
    "sub.to_csv('dev_submission.csv', index=False)\n",
    "\n",
    "display(sub.head())\n",
    "print(sub.shape)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3103714,
     "sourceId": 31254,
     "sourceType": "competition"
    },
    {
     "datasetId": 1931827,
     "sourceId": 3618498,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 94922230,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30154,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
