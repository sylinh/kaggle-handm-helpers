{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"},{"sourceId":14318670,"sourceType":"datasetVersion","datasetId":9140518}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\nimport os\nimport sys\nimport copy\nfrom datetime import datetime\nimport gc\nimport pickle as pkl\nimport shelve\n\nimport pandas as pd\nimport numpy as np\nimport cudf\n    \nsys.path.append(\"../input/\")\nfrom handmhelpers import io as h_io, sub as h_sub, cv as h_cv, fe as h_fe\nfrom handmhelpers import modeling as h_modeling, candidates as h_can, pairs as h_pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T03:56:10.952663Z","iopub.execute_input":"2025-12-29T03:56:10.953079Z","iopub.status.idle":"2025-12-29T03:56:15.107527Z","shell.execute_reply.started":"2025-12-29T03:56:10.953048Z","shell.execute_reply":"2025-12-29T03:56:15.106755Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 3.66 s, sys: 695 ms, total: 4.35 s\nWall time: 4.15 s\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Load and convert data","metadata":{}},{"cell_type":"code","source":"from datetime import timedelta\nimport cudf\nimport numpy as np\n\ndef patched_day_week_numbers(dates: cudf.Series):\n    pd_dates = cudf.to_datetime(dates)\n    unique_dates = cudf.Series(pd_dates.unique())\n    numbered_days = unique_dates - unique_dates.min() + timedelta(1)\n    numbered_days = numbered_days.dt.days\n    extra_days = numbered_days.max() % 7\n    numbered_days -= extra_days\n    day_weeks = (numbered_days + 6) // 7  # không dùng applymap\n    day_weeks_map = cudf.DataFrame({\"day_weeks\": day_weeks, \"unique_dates\": unique_dates}).set_index(\"unique_dates\")[\"day_weeks\"]\n    all_day_weeks = pd_dates.map(day_weeks_map).astype(\"int8\")\n    return all_day_weeks\n\nimport handmhelpers.fe as h_fe\nh_fe.day_week_numbers = patched_day_week_numbers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T03:56:15.108814Z","iopub.execute_input":"2025-12-29T03:56:15.109065Z","iopub.status.idle":"2025-12-29T03:56:15.114276Z","shell.execute_reply.started":"2025-12-29T03:56:15.109039Z","shell.execute_reply":"2025-12-29T03:56:15.113679Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%time\n\nc, t, a = h_io.load_data(files=['customers.csv', 'transactions_train.csv', 'articles.csv'])        \n\nindex_to_id_dict_path = h_fe.reduce_customer_id_memory(c, [t])\nt[\"week_number\"] = h_fe.day_week_numbers(t[\"t_dat\"])\nt[\"t_dat\"] = h_fe.day_numbers(t[\"t_dat\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T03:56:15.115000Z","iopub.execute_input":"2025-12-29T03:56:15.115260Z","iopub.status.idle":"2025-12-29T03:56:19.012616Z","shell.execute_reply.started":"2025-12-29T03:56:15.115237Z","shell.execute_reply":"2025-12-29T03:56:19.011913Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 3.41 s, sys: 1.38 s, total: 4.79 s\nWall time: 3.88 s\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Get item pairs","metadata":{}},{"cell_type":"code","source":"%%time\n# Tạo cặp bài viết cho nhiều tuần lịch sử (94–104) với 15 cặp mỗi bài để nâng recall ứng viên\npairs_per_item = 15\n\nweek_number_pairs = {}\nfor week_number in [94,95,96, 97, 98, 99, 100, 101, 102, 103, 104]:\n    print(f\"Creating pairs for week number {week_number}\")\n    week_number_pairs[week_number] = h_pairs.create_pairs(\n        t, week_number, pairs_per_item, verbose=False\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T03:56:19.014402Z","iopub.execute_input":"2025-12-29T03:56:19.014698Z","iopub.status.idle":"2025-12-29T03:56:56.941649Z","shell.execute_reply.started":"2025-12-29T03:56:19.014673Z","shell.execute_reply":"2025-12-29T03:56:56.940987Z"}},"outputs":[{"name":"stdout","text":"Creating pairs for week number 94\nCreating pairs for week number 95\nCreating pairs for week number 96\nCreating pairs for week number 97\nCreating pairs for week number 98\nCreating pairs for week number 99\nCreating pairs for week number 100\nCreating pairs for week number 101\nCreating pairs for week number 102\nCreating pairs for week number 103\nCreating pairs for week number 104\nCPU times: user 29.3 s, sys: 8.6 s, total: 37.9 s\nWall time: 37.9 s\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Main retrieval/features function!","metadata":{}},{"cell_type":"code","source":"def create_candidates_with_features_df(t, c, a, customer_batch=None, **kwargs):\n    # Tách dữ liệu theo tuần label: features dùng các tuần trước, label là tuần đích\n    features_df, label_df = h_cv.feature_label_split(\n        t, kwargs[\"label_week\"], kwargs[\"feature_periods\"]\n    )\n    # Đưa thời gian về dạng “cách đây bao nhiêu ngày/tuần” để model không lộ lịch tuyệt đối\n    features_df[\"t_dat\"] = h_fe.how_many_ago(features_df[\"t_dat\"])\n    features_df[\"week_number\"] = h_fe.how_many_ago(features_df[\"week_number\"])\n\n    # Lấy bảng cặp bài viết của tuần ngay trước tuần label\n    article_pairs_df = week_number_pairs[kwargs[\"label_week\"] - 1]\n\n    # Xác định tập khách hàng cần xử lý (full, batch, hoặc chỉ khách có label)\n    if len(label_df) > 0:\n        customers = label_df[\"customer_id\"].unique()\n    elif customer_batch is not None:\n        customers = customer_batch\n    else:\n        customers = None\n\n    # ----- Tạo ứng viên từ nhiều nguồn và lưu đặc trưng rule -----\n    features_db = {}\n\n    # Ứng viên mua gần đây (ngắn hạn)\n    recent_customer_cand, features_db[\"customer_article\"] = h_can.create_recent_customer_candidates(\n        features_df, kwargs[\"ca_num_weeks\"], customers=customers\n    )\n\n    # Ứng viên tuần gần nhất + cặp lift theo tuần\n    (\n        cust_last_week_cand,\n        cust_last_week_pair_cand,\n        features_db[\"clw\"],\n        features_db[\"clw_pairs\"],\n    ) = h_can.create_last_customer_weeks_and_pairs(\n        features_df, article_pairs_df, kwargs[\"clw_num_weeks\"], kwargs[\"clw_num_pair_weeks\"], customers=customers\n    )\n\n    # Ứng viên phổ biến theo hierarchy\n    popular_can, features_db[\"popular_articles\"] = h_can.create_popular_article_cand(\n        features_df, c, a, kwargs[\"pa_num_weeks\"], kwargs[\"hier_col\"],\n        num_candidates=kwargs[\"num_recent_candidates\"],\n        num_articles=kwargs[\"num_recent_articles\"],\n        customers=customers,\n    )\n\n    # Ứng viên theo bucket tuổi\n    (\n        age_bucket_can,\n        age_bucket_cust_features,\n        age_bucket_pair_features,\n    ) = h_can.create_age_bucket_candidates(\n        features_df, c, kwargs[\"num_age_buckets\"], articles=kwargs[\"num_recent_articles\"], customers=customers\n    )\n    features_db[\"age_bucket\"] = age_bucket_pair_features\n\n    # Gom rule-score từ từng nguồn ứng viên thành bảng rule_features_df\n    def build_rule_part(cand_df, feature_tuple, score_col, rule_name):\n        feature_df = feature_tuple[1].reset_index()[[\"customer_id\", \"article_id\", score_col]]\n        tmp = cand_df.merge(feature_df, on=[\"customer_id\", \"article_id\"], how=\"left\")\n        tmp = tmp.rename(columns={score_col: \"rule_score\"})\n        tmp[\"rule_score\"] = tmp[\"rule_score\"].fillna(-1)\n        tmp[\"rule\"] = rule_name\n        return tmp[[\"customer_id\", \"article_id\", \"rule\", \"rule_score\"]]\n\n    rule_parts = [\n        build_rule_part(recent_customer_cand, features_db[\"customer_article\"], \"ca_purchase_count\", \"recent\"),\n        build_rule_part(cust_last_week_cand, features_db[\"clw\"], \"ca_count\", \"last_weeks\"),\n        build_rule_part(cust_last_week_pair_cand, features_db[\"clw_pairs\"], \"pair_lift\", \"pairs\"),\n        build_rule_part(age_bucket_can, features_db[\"age_bucket\"], \"article_bucket_count\", \"age_bucket\"),\n    ]\n    # NEW: rule “popular” dùng recent_popularity_counts, khóa theo article_id\n    pop_feature_df = features_db[\"popular_articles\"][1].reset_index()[[\"article_id\", \"recent_popularity_counts\"]]\n    pop_rule = popular_can.merge(pop_feature_df, on=\"article_id\", how=\"left\")\n    pop_rule = pop_rule.rename(columns={\"recent_popularity_counts\": \"rule_score\"})\n    pop_rule[\"rule_score\"] = pop_rule[\"rule_score\"].fillna(-1)\n    pop_rule[\"rule\"] = \"popular\"\n    rule_parts.append(pop_rule[[\"customer_id\", \"article_id\", \"rule\", \"rule_score\"]])\n\n    rule_df = cudf.concat(rule_parts).sort_values(\n        [\"rule\", \"customer_id\", \"rule_score\"], ascending=[True, True, False]\n    )\n    rule_df[\"rank_within_rule\"] = rule_df.groupby([\"rule\", \"customer_id\"]).cumcount()\n\n    rule_features_df = (\n        rule_df.groupby([\"customer_id\", \"article_id\"])\n        .agg({\"rule\": \"nunique\", \"rule_score\": \"max\", \"rank_within_rule\": \"min\"})\n        .reset_index()\n    )\n    rule_features_df.columns = [\"customer_id\", \"article_id\", \"n_sources\", \"best_rule_score\", \"best_rank_within_rule\"]\n\n    # Thêm cờ nguồn\n    for rule_name in rule_df[\"rule\"].unique().to_pandas():\n        flag_df = rule_df[rule_df[\"rule\"] == rule_name][[\"customer_id\", \"article_id\"]].drop_duplicates()\n        flag_df[f\"{rule_name}_flag\"] = 1\n        rule_features_df = rule_features_df.merge(flag_df, how=\"left\", on=[\"customer_id\", \"article_id\"])\n        rule_features_df[f\"{rule_name}_flag\"] = rule_features_df[f\"{rule_name}_flag\"].fillna(0).astype(\"int8\")\n\n    # Hợp nhất các nguồn ứng viên và lọc trùng\n    cand = cudf.concat([popular_can, recent_customer_cand, cust_last_week_cand, cust_last_week_pair_cand, age_bucket_can])\\\n              .drop_duplicates()\\\n              .sort_values([\"customer_id\", \"article_id\"])\\\n              .reset_index(drop=True)\n    del popular_can, recent_customer_cand, cust_last_week_cand, cust_last_week_pair_cand, age_bucket_can\n\n    cand = h_can.filter_candidates(cand, t, **kwargs)\n\n    # ----- Sinh thêm đặc trưng hành vi/giá/recency/lag -----\n    h_fe.create_cust_hier_features(features_df, a, kwargs[\"hier_cols\"], features_db)\n    h_fe.create_cust_hier_decay_features(features_df, a, kwargs[\"hier_cols\"], features_db,\n                                         decay_gamma=kwargs.get(\"hier_decay_gamma\", 0.3))\n    # Đổi tên cột decay cho nhất quán\n    for k, v in list(features_db.items()):\n        if k.endswith(\"_decay_features\"):\n            hier = k[len(\"cust_\"):-len(\"_decay_features\")]\n            cols, df = v\n            df = df.rename(columns={\"last_seen_category_weeks_ago\": f\"last_seen_{hier}_weeks_ago\"})\n            features_db[k] = (cols, df)\n\n    h_fe.create_price_features(features_df, features_db)\n    h_fe.create_cust_features(c, features_db)\n    h_fe.create_article_cust_features(features_df, c, features_db)\n    h_fe.create_lag_features(features_df, a, kwargs[\"lag_days\"], features_db)\n    h_fe.create_rebuy_features(features_df, features_db)\n    h_fe.create_cust_t_features(features_df, a, features_db)\n    # Tương thích với fe.py bản gốc (2 tham số) và bản mới (3 tham số)\n    try:\n        h_fe.create_art_t_features(features_df, a, features_db)\n    except TypeError:\n        h_fe.create_art_t_features(features_df, features_db)\n    del features_df\n\n\n    # Giới hạn lại tập ứng viên nếu chỉ chạy trên subset khách\n    if customers is not None:\n        cand = cand[cand[\"customer_id\"].isin(customers)]\n\n    # Báo cáo recall/precision ứng viên trên CV\n    if kwargs[\"cv\"]:\n        ground_truth_candidates = label_df[[\"customer_id\", \"article_id\"]].drop_duplicates()\n        h_cv.report_candidates(cand, ground_truth_candidates)\n        del ground_truth_candidates\n\n    # Gắn đặc trưng vào ứng viên\n    cand_with_f_df = h_can.add_features_to_candidates(cand, features_db, c, a)\n    cand_with_f_df = cand_with_f_df.merge(rule_features_df, how=\"left\", on=[\"customer_id\", \"article_id\"])\n\n    # Thêm cột article thủ công\n    for article_col in kwargs[\"article_columns\"]:\n        art_col_map = a.set_index(\"article_id\")[article_col]\n        cand_with_f_df[article_col] = cand_with_f_df[\"article_id\"].map(art_col_map)\n\n    # Fill giá trị rule cho ứng viên chỉ đến từ popular (tránh NaN)\n    rule_fill = {\n        \"n_sources\": 0,\n        \"best_rule_score\": -1,\n        \"best_rank_within_rule\": 127,\n        \"recent_flag\": 0, \"last_weeks_flag\": 0, \"pairs_flag\": 0, \"age_bucket_flag\": 0, \"popular_flag\": 0,\n    }\n    cand_with_f_df = cand_with_f_df.fillna(rule_fill)\n\n    # Target encode các cột category dựa trên best_rule_score (nếu có), tránh mã thứ tự tùy ý\n    target_col = \"best_rule_score\" if \"best_rule_score\" in cand_with_f_df.columns else None\n    for col in cand_with_f_df.columns:\n        if col in [\"customer_id\", \"article_id\"]:\n            continue\n        if str(cand_with_f_df[col].dtype) not in [\"int8\",\"int16\",\"int32\",\"int64\",\"float16\",\"float32\",\"float64\",\"bool\"]:\n            if target_col:\n                te = cand_with_f_df.groupby(col)[target_col].mean()\n                cand_with_f_df[col] = cand_with_f_df[col].map(te).fillna(0).astype(\"float32\")\n            else:\n                cand_with_f_df[col] = cand_with_f_df[col].astype(\"category\").cat.codes.astype(\"float32\")\n\n    # Giữ subset đặc trưng nếu được chọn sẵn\n    if kwargs[\"selected_features\"] is not None:\n        cand_with_f_df = cand_with_f_df[[\"customer_id\", \"article_id\"] + kwargs[\"selected_features\"]]\n\n    assert len(cand) == len(cand_with_f_df), \"seem to have duplicates in the feature dfs\"\n    del cand\n\n    return cand_with_f_df, label_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T03:56:56.942687Z","iopub.execute_input":"2025-12-29T03:56:56.943040Z","iopub.status.idle":"2025-12-29T03:56:56.964296Z","shell.execute_reply.started":"2025-12-29T03:56:56.943016Z","shell.execute_reply":"2025-12-29T03:56:56.963771Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def calculate_model_score(ids_df, preds, truth_df):\n    predictions = h_modeling.create_predictions(ids_df, preds)\n    true_labels = h_cv.ground_truth(truth_df).set_index(\"customer_id\")[\"prediction\"]\n    score = round(h_cv.comp_average_precision(true_labels, predictions),5)\n    \n    return score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T03:56:56.965237Z","iopub.execute_input":"2025-12-29T03:56:56.965602Z","iopub.status.idle":"2025-12-29T03:56:56.987730Z","shell.execute_reply.started":"2025-12-29T03:56:56.965571Z","shell.execute_reply":"2025-12-29T03:56:56.987045Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Parameters - one place for all!","metadata":{}},{"cell_type":"code","source":"# Cấu hình chạy CV (đa tuần, tăng ứng viên/estimators)\ncv_params = {\n    \"cv\": True,                     # bật báo cáo recall ứng viên\n    \"feature_periods\": 105,         # dùng 105 tuần lịch sử cho feature\n    \"label_week\": 104,              # tuần label mặc định, sẽ bị override bởi cv_weeks\n    \"index_to_id_dict_path\": index_to_id_dict_path,\n    \"pairs_file_version\": \"_v3_5_ex\",\n    \"num_recent_candidates\": 80,    # NEW: tăng số ứng viên recent để nâng recall\n    \"num_recent_articles\": 20,      # NEW: tăng bài phổ biến/age bucket\n    \"hier_col\": \"department_no\",\n    \"ca_num_weeks\": 3,\n    \"clw_num_weeks\": 12,\n    \"clw_num_pair_weeks\": 2,\n    \"pa_num_weeks\": 2,              # NEW: kéo dài phổ biến thêm 2 tuần\n    \"num_age_buckets\": 4,\n    \"filter_recent_art_weeks\": 1,\n    \"filter_num_articles\": None,\n    \"lag_days\": [1, 3, 7, 14, 28],\n    \"article_columns\": [\"index_code\"],\n    \"hier_cols\": [\n        \"department_no\", \"section_no\", \"index_group_no\", \"index_code\",\n        \"product_type_no\", \"product_group_name\"\n    ],\n    \"hier_decay_gamma\": 0.3,\n    \"selected_features\": None,\n    \"lgbm_params\": {                # NEW: tăng capacity model cho CV\n        \"n_estimators\": 400,\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 64,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n    },\n    \"log_evaluation\": 10,\n    \"early_stopping\": 30,\n    \"eval_at\": 12,\n    \"save_model\": True,\n    \"num_concats\": 5,               # ghép 5 tuần train để tăng dữ liệu\n}\n\n# Cấu hình train/predict submit (tương tự CV nhưng n_estimators cao hơn, predict ensembe 2 model)\nsub_params = {\n    \"cv\": False,\n    \"feature_periods\": 105,\n    \"label_week\": 105,\n    \"index_to_id_dict_path\": index_to_id_dict_path,\n    \"pairs_file_version\": \"_v3_5_ex\",\n    \"num_recent_candidates\": 80,\n    \"num_recent_articles\": 20,\n    \"hier_col\": \"department_no\",\n    \"ca_num_weeks\": 3,\n    \"clw_num_weeks\": 12,\n    \"clw_num_pair_weeks\": 2,\n    \"pa_num_weeks\": 2,\n    \"num_age_buckets\": 4,\n    \"filter_recent_art_weeks\": 1,\n    \"filter_num_articles\": None,\n    \"lag_days\": [1, 3, 7, 14, 28],\n    \"article_columns\": [\"index_code\"],\n    \"hier_cols\": [\n        \"department_no\", \"section_no\", \"index_group_no\", \"index_code\",\n        \"product_type_no\", \"product_group_name\"\n    ],\n    \"hier_decay_gamma\": 0.3,\n    \"selected_features\": None,\n    \"lgbm_params\": {                # NEW: nhiều cây hơn cho submit\n        \"n_estimators\": 500,\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 64,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n    },\n    \"log_evaluation\": 10,\n    \"eval_at\": 12,\n    \"prediction_models\": [\"model_104\", \"model_105\"],  # ensembe 2 model\n    \"save_model\": True,\n    \"num_concats\": 5,\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T03:56:56.988680Z","iopub.execute_input":"2025-12-29T03:56:56.988903Z","iopub.status.idle":"2025-12-29T03:56:57.004405Z","shell.execute_reply.started":"2025-12-29T03:56:56.988883Z","shell.execute_reply":"2025-12-29T03:56:57.003916Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"cand_features_func = create_candidates_with_features_df\nscoring_func = calculate_model_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T03:56:57.005221Z","iopub.execute_input":"2025-12-29T03:56:57.005475Z","iopub.status.idle":"2025-12-29T03:56:57.022634Z","shell.execute_reply.started":"2025-12-29T03:56:57.005448Z","shell.execute_reply":"2025-12-29T03:56:57.022109Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"%%time\n# Chạy cross-validation cho các tuần 102–104 (đa tuần để ổn định hơn so với 1 tuần)\ncv_weeks = [102, 103, 104]\nresults = h_modeling.run_all_cvs(\n    t, c, a, cand_features_func, scoring_func,\n    cv_weeks=cv_weeks, **cv_params\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T03:56:57.023417Z","iopub.execute_input":"2025-12-29T03:56:57.023875Z","iopub.status.idle":"2025-12-29T04:27:35.841689Z","shell.execute_reply.started":"2025-12-29T03:56:57.023842Z","shell.execute_reply":"2025-12-29T04:27:35.840993Z"}},"outputs":[{"name":"stdout","text":"preparing training modeling dfs for 101...\ncandidates recall: 10.80% (27,552/255,172)\ncandidates precision: 0.53% (27,552/5,195,622)\npreparing training modeling dfs for 100...\ncandidates recall: 10.35% (23,898/230,825)\ncandidates precision: 0.47% (23,898/5,032,850)\npreparing training modeling dfs for 99...\ncandidates recall: 10.23% (24,268/237,160)\ncandidates precision: 0.48% (24,268/5,048,311)\npreparing training modeling dfs for 98...\ncandidates recall: 9.60% (24,910/259,512)\ncandidates precision: 0.47% (24,910/5,340,723)\npreparing training modeling dfs for 97...\ncandidates recall: 8.75% (25,185/287,700)\ncandidates precision: 0.45% (25,185/5,623,680)\nconcatenating all weeks together\npreparing evaluation modeling dfs...\ncandidates recall: 11.48% (27,339/238,074)\ncandidates precision: 0.57% (27,339/4,837,473)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Total groups: 90696, total data: 7484408\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.318732 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 12609\n[LightGBM] [Info] Number of data points in the train set: 7484408, number of used features: 72\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Total groups: 20025, total data: 1494069\nTraining until validation scores don't improve for 30 rounds\n[10]\ttrain's map@12: 0.209913\ttrain's ndcg@12: 0.283198\tvalidation's map@12: 0.203146\tvalidation's ndcg@12: 0.273933\n[20]\ttrain's map@12: 0.214855\ttrain's ndcg@12: 0.289346\tvalidation's map@12: 0.204602\tvalidation's ndcg@12: 0.276156\n[30]\ttrain's map@12: 0.218293\ttrain's ndcg@12: 0.29343\tvalidation's map@12: 0.205199\tvalidation's ndcg@12: 0.277347\n[40]\ttrain's map@12: 0.221054\ttrain's ndcg@12: 0.296943\tvalidation's map@12: 0.2057\tvalidation's ndcg@12: 0.277942\n[50]\ttrain's map@12: 0.224027\ttrain's ndcg@12: 0.30051\tvalidation's map@12: 0.206753\tvalidation's ndcg@12: 0.279179\n[60]\ttrain's map@12: 0.226568\ttrain's ndcg@12: 0.303717\tvalidation's map@12: 0.206765\tvalidation's ndcg@12: 0.279519\n[70]\ttrain's map@12: 0.229357\ttrain's ndcg@12: 0.306905\tvalidation's map@12: 0.207163\tvalidation's ndcg@12: 0.279834\n[80]\ttrain's map@12: 0.231809\ttrain's ndcg@12: 0.309864\tvalidation's map@12: 0.207497\tvalidation's ndcg@12: 0.280833\n[90]\ttrain's map@12: 0.233889\ttrain's ndcg@12: 0.312204\tvalidation's map@12: 0.208493\tvalidation's ndcg@12: 0.281675\n[100]\ttrain's map@12: 0.235835\ttrain's ndcg@12: 0.314428\tvalidation's map@12: 0.209422\tvalidation's ndcg@12: 0.28273\n[110]\ttrain's map@12: 0.237671\ttrain's ndcg@12: 0.316561\tvalidation's map@12: 0.209722\tvalidation's ndcg@12: 0.283397\n[120]\ttrain's map@12: 0.239846\ttrain's ndcg@12: 0.318881\tvalidation's map@12: 0.210124\tvalidation's ndcg@12: 0.283949\n[130]\ttrain's map@12: 0.241596\ttrain's ndcg@12: 0.32083\tvalidation's map@12: 0.210543\tvalidation's ndcg@12: 0.284466\n[140]\ttrain's map@12: 0.243301\ttrain's ndcg@12: 0.32266\tvalidation's map@12: 0.21061\tvalidation's ndcg@12: 0.284542\n[150]\ttrain's map@12: 0.244843\ttrain's ndcg@12: 0.324493\tvalidation's map@12: 0.210772\tvalidation's ndcg@12: 0.284871\n[160]\ttrain's map@12: 0.246615\ttrain's ndcg@12: 0.326434\tvalidation's map@12: 0.211214\tvalidation's ndcg@12: 0.285081\n[170]\ttrain's map@12: 0.2483\ttrain's ndcg@12: 0.328283\tvalidation's map@12: 0.211305\tvalidation's ndcg@12: 0.285134\n[180]\ttrain's map@12: 0.25\ttrain's ndcg@12: 0.33015\tvalidation's map@12: 0.211689\tvalidation's ndcg@12: 0.285567\n[190]\ttrain's map@12: 0.251761\ttrain's ndcg@12: 0.33191\tvalidation's map@12: 0.212183\tvalidation's ndcg@12: 0.286214\n[200]\ttrain's map@12: 0.253523\ttrain's ndcg@12: 0.333783\tvalidation's map@12: 0.212188\tvalidation's ndcg@12: 0.286388\n[210]\ttrain's map@12: 0.255203\ttrain's ndcg@12: 0.335542\tvalidation's map@12: 0.212256\tvalidation's ndcg@12: 0.286379\n[220]\ttrain's map@12: 0.256997\ttrain's ndcg@12: 0.337435\tvalidation's map@12: 0.21285\tvalidation's ndcg@12: 0.286673\n[230]\ttrain's map@12: 0.258473\ttrain's ndcg@12: 0.33896\tvalidation's map@12: 0.212747\tvalidation's ndcg@12: 0.286672\n[240]\ttrain's map@12: 0.259894\ttrain's ndcg@12: 0.340413\tvalidation's map@12: 0.213027\tvalidation's ndcg@12: 0.286779\n[250]\ttrain's map@12: 0.261506\ttrain's ndcg@12: 0.34197\tvalidation's map@12: 0.213133\tvalidation's ndcg@12: 0.286933\n[260]\ttrain's map@12: 0.262902\ttrain's ndcg@12: 0.34344\tvalidation's map@12: 0.213215\tvalidation's ndcg@12: 0.286978\n[270]\ttrain's map@12: 0.26452\ttrain's ndcg@12: 0.345126\tvalidation's map@12: 0.213474\tvalidation's ndcg@12: 0.287155\n[280]\ttrain's map@12: 0.266043\ttrain's ndcg@12: 0.346729\tvalidation's map@12: 0.212944\tvalidation's ndcg@12: 0.286715\n[290]\ttrain's map@12: 0.267243\ttrain's ndcg@12: 0.347948\tvalidation's map@12: 0.212855\tvalidation's ndcg@12: 0.286564\nEarly stopping, best iteration is:\n[267]\ttrain's map@12: 0.263936\ttrain's ndcg@12: 0.344585\tvalidation's map@12: 0.213503\tvalidation's ndcg@12: 0.287082\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/../input/handmhelpers/modeling.py:272: FutureWarning: Parameter `output_class` was deprecated in version 25.06 and will be replaced with `is_classifier` in 25.08. Please use `is_classifier` in the future. For now, `output_class` parameter has been automatically converted to `is_classifier`.\n  model = ForestInference.load(model_path, model_type=\"lightgbm\", output_class=False)\n","output_type":"stream"},{"name":"stdout","text":"Train AUC 0.7822\nTrain score:  0.07055\nEval AUC 0.7364\nEval score: 0.03428\n\n\nlast_weeks_flag         0\npairs_flag              0\nrecent_flag             0\npopular_flag            3\nage_bucket_flag         5\n                     ... \nnewness_days          481\nrebuy_count_ratio     489\nart_sales_channel     515\ncust_sales_channel    520\nlast_1_days_count     627\nLength: 72, dtype: int32\nFinished cv of week 102 in 0:11:16.207741. Score: 0.03428\n\npreparing training modeling dfs for 102...\ncandidates recall: 11.48% (27,339/238,074)\ncandidates precision: 0.57% (27,339/4,837,472)\npreparing training modeling dfs for 101...\ncandidates recall: 10.80% (27,552/255,172)\ncandidates precision: 0.53% (27,552/5,195,631)\npreparing training modeling dfs for 100...\ncandidates recall: 10.35% (23,896/230,825)\ncandidates precision: 0.47% (23,896/5,032,739)\npreparing training modeling dfs for 99...\ncandidates recall: 10.23% (24,268/237,160)\ncandidates precision: 0.48% (24,268/5,048,315)\npreparing training modeling dfs for 98...\ncandidates recall: 9.60% (24,910/259,512)\ncandidates precision: 0.47% (24,910/5,340,720)\nconcatenating all weeks together\npreparing evaluation modeling dfs...\ncandidates recall: 11.12% (25,340/227,910)\ncandidates precision: 0.55% (25,340/4,646,861)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Total groups: 92215, total data: 7460622\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.196570 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 12662\n[LightGBM] [Info] Number of data points in the train set: 7460622, number of used features: 72\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Total groups: 18529, total data: 1412065\nTraining until validation scores don't improve for 30 rounds\n[10]\ttrain's map@12: 0.211809\ttrain's ndcg@12: 0.285179\tvalidation's map@12: 0.20949\tvalidation's ndcg@12: 0.280783\n[20]\ttrain's map@12: 0.217221\ttrain's ndcg@12: 0.291657\tvalidation's map@12: 0.212979\tvalidation's ndcg@12: 0.285222\n[30]\ttrain's map@12: 0.219699\ttrain's ndcg@12: 0.295129\tvalidation's map@12: 0.213795\tvalidation's ndcg@12: 0.286328\n[40]\ttrain's map@12: 0.222731\ttrain's ndcg@12: 0.29856\tvalidation's map@12: 0.214882\tvalidation's ndcg@12: 0.287755\n[50]\ttrain's map@12: 0.225501\ttrain's ndcg@12: 0.302098\tvalidation's map@12: 0.216651\tvalidation's ndcg@12: 0.289972\n[60]\ttrain's map@12: 0.228173\ttrain's ndcg@12: 0.305144\tvalidation's map@12: 0.217448\tvalidation's ndcg@12: 0.291135\n[70]\ttrain's map@12: 0.230653\ttrain's ndcg@12: 0.308339\tvalidation's map@12: 0.21838\tvalidation's ndcg@12: 0.292067\n[80]\ttrain's map@12: 0.232759\ttrain's ndcg@12: 0.310934\tvalidation's map@12: 0.219367\tvalidation's ndcg@12: 0.293572\n[90]\ttrain's map@12: 0.234876\ttrain's ndcg@12: 0.313394\tvalidation's map@12: 0.220161\tvalidation's ndcg@12: 0.294493\n[100]\ttrain's map@12: 0.236882\ttrain's ndcg@12: 0.315771\tvalidation's map@12: 0.220614\tvalidation's ndcg@12: 0.295029\n[110]\ttrain's map@12: 0.238666\ttrain's ndcg@12: 0.317768\tvalidation's map@12: 0.221336\tvalidation's ndcg@12: 0.295927\n[120]\ttrain's map@12: 0.240551\ttrain's ndcg@12: 0.319898\tvalidation's map@12: 0.221775\tvalidation's ndcg@12: 0.296443\n[130]\ttrain's map@12: 0.242071\ttrain's ndcg@12: 0.321707\tvalidation's map@12: 0.222655\tvalidation's ndcg@12: 0.297278\n[140]\ttrain's map@12: 0.243845\ttrain's ndcg@12: 0.323576\tvalidation's map@12: 0.222926\tvalidation's ndcg@12: 0.297569\n[150]\ttrain's map@12: 0.245612\ttrain's ndcg@12: 0.325393\tvalidation's map@12: 0.223668\tvalidation's ndcg@12: 0.298414\n[160]\ttrain's map@12: 0.247411\ttrain's ndcg@12: 0.327313\tvalidation's map@12: 0.223519\tvalidation's ndcg@12: 0.298212\n[170]\ttrain's map@12: 0.248938\ttrain's ndcg@12: 0.329088\tvalidation's map@12: 0.223738\tvalidation's ndcg@12: 0.298391\n[180]\ttrain's map@12: 0.250526\ttrain's ndcg@12: 0.330657\tvalidation's map@12: 0.223899\tvalidation's ndcg@12: 0.298565\n[190]\ttrain's map@12: 0.252113\ttrain's ndcg@12: 0.332384\tvalidation's map@12: 0.223903\tvalidation's ndcg@12: 0.298666\n[200]\ttrain's map@12: 0.2537\ttrain's ndcg@12: 0.334009\tvalidation's map@12: 0.223577\tvalidation's ndcg@12: 0.298513\n[210]\ttrain's map@12: 0.255458\ttrain's ndcg@12: 0.33584\tvalidation's map@12: 0.223666\tvalidation's ndcg@12: 0.298235\nEarly stopping, best iteration is:\n[182]\ttrain's map@12: 0.25083\ttrain's ndcg@12: 0.331009\tvalidation's map@12: 0.224102\tvalidation's ndcg@12: 0.298893\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/../input/handmhelpers/modeling.py:272: FutureWarning: Parameter `output_class` was deprecated in version 25.06 and will be replaced with `is_classifier` in 25.08. Please use `is_classifier` in the future. For now, `output_class` parameter has been automatically converted to `is_classifier`.\n  model = ForestInference.load(model_path, model_type=\"lightgbm\", output_class=False)\n","output_type":"stream"},{"name":"stdout","text":"Train AUC 0.7769\nTrain score:  0.07176\nEval AUC 0.7484\nEval score: 0.03481\n\n\nlast_weeks_flag         0\npairs_flag              0\nrecent_flag             0\npopular_flag            1\nage_bucket_flag         3\n                     ... \nrebuy_count_ratio     348\ncust_sales_channel    356\nart_sales_channel     381\nnewness_days          390\nlast_1_days_count     473\nLength: 72, dtype: int32\nFinished cv of week 103 in 0:08:52.063468. Score: 0.03481\n\npreparing training modeling dfs for 103...\ncandidates recall: 11.12% (25,340/227,910)\ncandidates precision: 0.55% (25,340/4,646,857)\npreparing training modeling dfs for 102...\ncandidates recall: 11.48% (27,339/238,074)\ncandidates precision: 0.57% (27,339/4,837,473)\npreparing training modeling dfs for 101...\ncandidates recall: 10.80% (27,552/255,172)\ncandidates precision: 0.53% (27,552/5,195,624)\npreparing training modeling dfs for 100...\ncandidates recall: 10.35% (23,898/230,825)\ncandidates precision: 0.47% (23,898/5,032,853)\npreparing training modeling dfs for 99...\ncandidates recall: 10.23% (24,268/237,160)\ncandidates precision: 0.48% (24,268/5,048,311)\nconcatenating all weeks together\npreparing evaluation modeling dfs...\ncandidates recall: 12.22% (26,116/213,728)\ncandidates precision: 0.61% (26,116/4,313,825)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Total groups: 92920, total data: 7330117\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.312735 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 12658\n[LightGBM] [Info] Number of data points in the train set: 7330117, number of used features: 72\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Total groups: 18468, total data: 1363738\nTraining until validation scores don't improve for 30 rounds\n[10]\ttrain's map@12: 0.216159\ttrain's ndcg@12: 0.291153\tvalidation's map@12: 0.200488\tvalidation's ndcg@12: 0.272107\n[20]\ttrain's map@12: 0.221426\ttrain's ndcg@12: 0.297235\tvalidation's map@12: 0.205163\tvalidation's ndcg@12: 0.276859\n[30]\ttrain's map@12: 0.224809\ttrain's ndcg@12: 0.301306\tvalidation's map@12: 0.205725\tvalidation's ndcg@12: 0.277871\n[40]\ttrain's map@12: 0.227847\ttrain's ndcg@12: 0.304924\tvalidation's map@12: 0.206914\tvalidation's ndcg@12: 0.279609\n[50]\ttrain's map@12: 0.230488\ttrain's ndcg@12: 0.308199\tvalidation's map@12: 0.207764\tvalidation's ndcg@12: 0.280933\n[60]\ttrain's map@12: 0.233018\ttrain's ndcg@12: 0.311115\tvalidation's map@12: 0.209653\tvalidation's ndcg@12: 0.2829\n[70]\ttrain's map@12: 0.235162\ttrain's ndcg@12: 0.313471\tvalidation's map@12: 0.210657\tvalidation's ndcg@12: 0.284506\n[80]\ttrain's map@12: 0.237518\ttrain's ndcg@12: 0.316284\tvalidation's map@12: 0.211074\tvalidation's ndcg@12: 0.285116\n[90]\ttrain's map@12: 0.239347\ttrain's ndcg@12: 0.318363\tvalidation's map@12: 0.211602\tvalidation's ndcg@12: 0.285882\n[100]\ttrain's map@12: 0.241394\ttrain's ndcg@12: 0.320753\tvalidation's map@12: 0.212444\tvalidation's ndcg@12: 0.28705\n[110]\ttrain's map@12: 0.243236\ttrain's ndcg@12: 0.322873\tvalidation's map@12: 0.213017\tvalidation's ndcg@12: 0.287868\n[120]\ttrain's map@12: 0.24517\ttrain's ndcg@12: 0.324866\tvalidation's map@12: 0.212887\tvalidation's ndcg@12: 0.287594\n[130]\ttrain's map@12: 0.247026\ttrain's ndcg@12: 0.327083\tvalidation's map@12: 0.213373\tvalidation's ndcg@12: 0.288191\n[140]\ttrain's map@12: 0.248771\ttrain's ndcg@12: 0.329003\tvalidation's map@12: 0.213321\tvalidation's ndcg@12: 0.288313\n[150]\ttrain's map@12: 0.250192\ttrain's ndcg@12: 0.330674\tvalidation's map@12: 0.213746\tvalidation's ndcg@12: 0.28899\n[160]\ttrain's map@12: 0.252046\ttrain's ndcg@12: 0.332566\tvalidation's map@12: 0.214404\tvalidation's ndcg@12: 0.289424\n[170]\ttrain's map@12: 0.253687\ttrain's ndcg@12: 0.33429\tvalidation's map@12: 0.214524\tvalidation's ndcg@12: 0.289648\n[180]\ttrain's map@12: 0.255226\ttrain's ndcg@12: 0.335992\tvalidation's map@12: 0.214391\tvalidation's ndcg@12: 0.289777\n[190]\ttrain's map@12: 0.256721\ttrain's ndcg@12: 0.337559\tvalidation's map@12: 0.21446\tvalidation's ndcg@12: 0.289761\n[200]\ttrain's map@12: 0.258374\ttrain's ndcg@12: 0.33941\tvalidation's map@12: 0.214468\tvalidation's ndcg@12: 0.289783\n[210]\ttrain's map@12: 0.260025\ttrain's ndcg@12: 0.341077\tvalidation's map@12: 0.214878\tvalidation's ndcg@12: 0.290067\n[220]\ttrain's map@12: 0.261653\ttrain's ndcg@12: 0.342756\tvalidation's map@12: 0.214862\tvalidation's ndcg@12: 0.290236\n[230]\ttrain's map@12: 0.263181\ttrain's ndcg@12: 0.344438\tvalidation's map@12: 0.214895\tvalidation's ndcg@12: 0.29058\n[240]\ttrain's map@12: 0.264547\ttrain's ndcg@12: 0.345793\tvalidation's map@12: 0.215311\tvalidation's ndcg@12: 0.290827\n[250]\ttrain's map@12: 0.266137\ttrain's ndcg@12: 0.347481\tvalidation's map@12: 0.214888\tvalidation's ndcg@12: 0.290143\n[260]\ttrain's map@12: 0.267652\ttrain's ndcg@12: 0.348963\tvalidation's map@12: 0.21489\tvalidation's ndcg@12: 0.290223\n[270]\ttrain's map@12: 0.268928\ttrain's ndcg@12: 0.350346\tvalidation's map@12: 0.214844\tvalidation's ndcg@12: 0.290258\nEarly stopping, best iteration is:\n[241]\ttrain's map@12: 0.264735\ttrain's ndcg@12: 0.34598\tvalidation's map@12: 0.215541\tvalidation's ndcg@12: 0.291086\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/../input/handmhelpers/modeling.py:272: FutureWarning: Parameter `output_class` was deprecated in version 25.06 and will be replaced with `is_classifier` in 25.08. Please use `is_classifier` in the future. For now, `output_class` parameter has been automatically converted to `is_classifier`.\n  model = ForestInference.load(model_path, model_type=\"lightgbm\", output_class=False)\n","output_type":"stream"},{"name":"stdout","text":"Train AUC 0.7814\nTrain score:  0.07534\nEval AUC 0.7367\nEval score: 0.03619\n\n\nlast_weeks_flag         0\npairs_flag              0\nrecent_flag             0\npopular_flag            1\nage_bucket_flag         6\n                     ... \ncust_sales_channel    455\nrebuy_count_ratio     466\nart_sales_channel     475\nnewness_days          492\nlast_1_days_count     501\nLength: 72, dtype: int32\nFinished cv of week 104 in 0:10:30.531203. Score: 0.03619\n\nFinished all 3 cvs in 0:30:38.802410. Average cv score: 0.03509\nCPU times: user 54min 44s, sys: 1min 13s, total: 55min 57s\nWall time: 30min 38s\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from cuml.fil import ForestInference as _FI\n\n_real_load = _FI.load\n\ndef _load_compat(*args, output_class=None, is_classifier=None, **kwargs):\n    # helper cũ truyền output_class -> map sang is_classifier\n    if is_classifier is None and output_class is not None:\n        is_classifier = output_class\n    return _real_load(*args, is_classifier=is_classifier, **kwargs)\n\n_FI.load = staticmethod(_load_compat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T04:46:46.242809Z","iopub.execute_input":"2025-12-29T04:46:46.243036Z","iopub.status.idle":"2025-12-29T04:46:46.247357Z","shell.execute_reply.started":"2025-12-29T04:46:46.243013Z","shell.execute_reply":"2025-12-29T04:46:46.246739Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=r\".*Parameter `output_class` was deprecated.*\",\n    category=FutureWarning,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T04:46:46.248176Z","iopub.execute_input":"2025-12-29T04:46:46.248397Z","iopub.status.idle":"2025-12-29T04:46:46.265943Z","shell.execute_reply.started":"2025-12-29T04:46:46.248375Z","shell.execute_reply":"2025-12-29T04:46:46.265376Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import math\nimport handmhelpers.modeling as h_modeling\n\ndef full_sub_predict_run_small_batches(t, c, a, cand_features_func, batch_splits=8, **kwargs):\n    # Chia khách hàng thành nhiều batch nhỏ để tránh OOM khi tạo feature/predict\n    customer_batches = []\n    n = len(c)\n    for i in range(batch_splits):\n        start = i * n // batch_splits\n        end = (i + 1) * n // batch_splits\n        customer_batches.append(c[start:end][\"customer_id\"].to_pandas().to_list())\n\n    batch_preds = []\n    for idx, customer_batch in enumerate(customer_batches):\n        print(f\"generating candidates/features for batch #{idx+1} of {len(customer_batches)}\")\n        sub_ids_df, sub_X = h_modeling.prepare_prediction_dfs(\n            t, c, a, cand_features_func, customer_batch=customer_batch, **kwargs\n        )\n        print(f\"candidate/features shape of batch: ({sub_X.shape[0]:,}, {sub_X.shape[1]})\")\n\n        # Ensemble các model trong prediction_models, trung bình điểm\n        model_paths = kwargs.get(\"prediction_models\")\n        model_nums = len(model_paths)\n        first_model = h_modeling.ForestInference.load(model_paths[0], model_type=\"lightgbm\", output_class=False)\n        sub_pred = h_modeling.pred_in_batches(first_model, sub_X) / model_nums\n        del first_model\n\n        for mp in model_paths[1:]:\n            m = h_modeling.ForestInference.load(mp, model_type=\"lightgbm\", output_class=False)\n            sub_pred += h_modeling.pred_in_batches(m, sub_X) / model_nums\n            del m\n\n        batch_preds.append(h_modeling.create_predictions(sub_ids_df, sub_pred))\n        del sub_ids_df, sub_X, sub_pred\n\n    return cudf.concat(batch_preds)\n\n# Ghi đè hàm predict gốc để dùng bản chia batch nhỏ\nh_modeling.full_sub_predict_run = full_sub_predict_run_small_batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T04:46:46.266799Z","iopub.execute_input":"2025-12-29T04:46:46.267090Z","iopub.status.idle":"2025-12-29T04:46:46.282500Z","shell.execute_reply.started":"2025-12-29T04:46:46.267059Z","shell.execute_reply":"2025-12-29T04:46:46.281742Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"%%time\ngc.collect()\n# Train full dữ liệu submit và lưu model theo sub_params\nh_modeling.full_sub_train_run(t, c, a, cand_features_func, scoring_func, **sub_params)\n# Predict theo batch nhỏ (batch_splits=8) để tránh OOM, dùng ensemble model_104/model_105\npredictions = h_modeling.full_sub_predict_run(\n    t, c, a, cand_features_func, batch_splits=8, **sub_params\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T04:27:35.883366Z","iopub.execute_input":"2025-12-29T04:27:35.883695Z","iopub.status.idle":"2025-12-29T04:46:09.894507Z","shell.execute_reply.started":"2025-12-29T04:27:35.883670Z","shell.execute_reply":"2025-12-29T04:46:09.893865Z"}},"outputs":[{"name":"stdout","text":"preparing training modeling dfs for 104...\npreparing training modeling dfs for 103...\npreparing training modeling dfs for 102...\npreparing training modeling dfs for 101...\npreparing training modeling dfs for 100...\nconcatenating all weeks together\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Total groups: 94821, total data: 7251356\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.174357 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 12682\n[LightGBM] [Info] Number of data points in the train set: 7251356, number of used features: 72\n[10]\ttrain's map@12: 0.213485\ttrain's ndcg@12: 0.288159\n[20]\ttrain's map@12: 0.219235\ttrain's ndcg@12: 0.294794\n[30]\ttrain's map@12: 0.223253\ttrain's ndcg@12: 0.299672\n[40]\ttrain's map@12: 0.22617\ttrain's ndcg@12: 0.30311\n[50]\ttrain's map@12: 0.228622\ttrain's ndcg@12: 0.306097\n[60]\ttrain's map@12: 0.230876\ttrain's ndcg@12: 0.308776\n[70]\ttrain's map@12: 0.233134\ttrain's ndcg@12: 0.311462\n[80]\ttrain's map@12: 0.235274\ttrain's ndcg@12: 0.314001\n[90]\ttrain's map@12: 0.237216\ttrain's ndcg@12: 0.316243\n[100]\ttrain's map@12: 0.23914\ttrain's ndcg@12: 0.318469\n[110]\ttrain's map@12: 0.240958\ttrain's ndcg@12: 0.320623\n[120]\ttrain's map@12: 0.242851\ttrain's ndcg@12: 0.322792\n[130]\ttrain's map@12: 0.244668\ttrain's ndcg@12: 0.324721\n[140]\ttrain's map@12: 0.246163\ttrain's ndcg@12: 0.326348\n[150]\ttrain's map@12: 0.247786\ttrain's ndcg@12: 0.328171\n[160]\ttrain's map@12: 0.249608\ttrain's ndcg@12: 0.330178\n[170]\ttrain's map@12: 0.251331\ttrain's ndcg@12: 0.332016\n[180]\ttrain's map@12: 0.252825\ttrain's ndcg@12: 0.333653\n[190]\ttrain's map@12: 0.254411\ttrain's ndcg@12: 0.335308\n[200]\ttrain's map@12: 0.256196\ttrain's ndcg@12: 0.337211\n[210]\ttrain's map@12: 0.257855\ttrain's ndcg@12: 0.338948\n[220]\ttrain's map@12: 0.259349\ttrain's ndcg@12: 0.340482\n[230]\ttrain's map@12: 0.26059\ttrain's ndcg@12: 0.341785\n[240]\ttrain's map@12: 0.262265\ttrain's ndcg@12: 0.343655\n[250]\ttrain's map@12: 0.263675\ttrain's ndcg@12: 0.345228\n[260]\ttrain's map@12: 0.265208\ttrain's ndcg@12: 0.346725\n[270]\ttrain's map@12: 0.266614\ttrain's ndcg@12: 0.348274\n[280]\ttrain's map@12: 0.268119\ttrain's ndcg@12: 0.349844\n[290]\ttrain's map@12: 0.26954\ttrain's ndcg@12: 0.351407\n[300]\ttrain's map@12: 0.271015\ttrain's ndcg@12: 0.352944\n[310]\ttrain's map@12: 0.272476\ttrain's ndcg@12: 0.354432\n[320]\ttrain's map@12: 0.274116\ttrain's ndcg@12: 0.356098\n[330]\ttrain's map@12: 0.275395\ttrain's ndcg@12: 0.357374\n[340]\ttrain's map@12: 0.276812\ttrain's ndcg@12: 0.358743\n[350]\ttrain's map@12: 0.277953\ttrain's ndcg@12: 0.359991\n[360]\ttrain's map@12: 0.279105\ttrain's ndcg@12: 0.361192\n[370]\ttrain's map@12: 0.280518\ttrain's ndcg@12: 0.362682\n[380]\ttrain's map@12: 0.281724\ttrain's ndcg@12: 0.363915\n[390]\ttrain's map@12: 0.283174\ttrain's ndcg@12: 0.365474\n[400]\ttrain's map@12: 0.284303\ttrain's ndcg@12: 0.366719\n[410]\ttrain's map@12: 0.285775\ttrain's ndcg@12: 0.368258\n[420]\ttrain's map@12: 0.28697\ttrain's ndcg@12: 0.369566\n[430]\ttrain's map@12: 0.288204\ttrain's ndcg@12: 0.370854\n[440]\ttrain's map@12: 0.289369\ttrain's ndcg@12: 0.372073\n[450]\ttrain's map@12: 0.29054\ttrain's ndcg@12: 0.373318\n[460]\ttrain's map@12: 0.291825\ttrain's ndcg@12: 0.37465\n[470]\ttrain's map@12: 0.292864\ttrain's ndcg@12: 0.375711\n[480]\ttrain's map@12: 0.294079\ttrain's ndcg@12: 0.377116\n[490]\ttrain's map@12: 0.295403\ttrain's ndcg@12: 0.378375\n[500]\ttrain's map@12: 0.296738\ttrain's ndcg@12: 0.37988\nTrain AUC 0.7894\nTrain score:  0.08096\ngenerating candidates/features for batch #1 of 8\ncandidate/features shape of batch: (8,837,641, 72)\ngenerating candidates/features for batch #2 of 8\ncandidate/features shape of batch: (8,810,644, 72)\ngenerating candidates/features for batch #3 of 8\ncandidate/features shape of batch: (8,820,574, 72)\ngenerating candidates/features for batch #4 of 8\ncandidate/features shape of batch: (8,822,750, 72)\ngenerating candidates/features for batch #5 of 8\ncandidate/features shape of batch: (8,831,836, 72)\ngenerating candidates/features for batch #6 of 8\ncandidate/features shape of batch: (8,821,821, 72)\ngenerating candidates/features for batch #7 of 8\ncandidate/features shape of batch: (8,811,925, 72)\ngenerating candidates/features for batch #8 of 8\ncandidate/features shape of batch: (8,811,950, 72)\nCPU times: user 30min 26s, sys: 1min 25s, total: 31min 51s\nWall time: 18min 33s\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"sub = h_sub.create_sub(c[\"customer_id\"], predictions, index_to_id_dict_path)\nsub.to_csv('dev_submission.csv', index=False)\n\ndisplay(sub.head())\nprint(sub.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T04:46:09.895374Z","iopub.execute_input":"2025-12-29T04:46:09.895662Z","iopub.status.idle":"2025-12-29T04:46:46.238292Z","shell.execute_reply.started":"2025-12-29T04:46:09.895636Z","shell.execute_reply":"2025-12-29T04:46:46.237519Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                         customer_id  \\\n0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   \n4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...   \n\n                                          prediction  \n0  0568601043 0779781015 0568601044 0858856005 07...  \n1  0448509014 0918522001 0714790020 0874110016 09...  \n2  0794321007 0794321008 0805000001 0918522001 09...  \n3  0861803009 0852584001 0918292001 0866731001 09...  \n4  0924243002 0730683050 0918522001 0791587001 07...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n      <td>0568601043 0779781015 0568601044 0858856005 07...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n      <td>0448509014 0918522001 0714790020 0874110016 09...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n      <td>0794321007 0794321008 0805000001 0918522001 09...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n      <td>0861803009 0852584001 0918292001 0866731001 09...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n      <td>0924243002 0730683050 0918522001 0791587001 07...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"(1371980, 2)\n","output_type":"stream"}],"execution_count":14}]}